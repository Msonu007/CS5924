{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"banner-chatgpt.png\" width=\"100%\">\n",
    "<br>\n",
    "\n",
    "To establish your OpenAI connection, run the first two cells.\n",
    "<br>\n",
    "To export your chat:\n",
    "* use File | Download As and\n",
    "  * choose Jupyter Notebook format (.ipynb) or\n",
    "  * choose Markdown format (.md) depending on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T18:15:23.545808998Z",
     "start_time": "2023-09-07T18:15:23.541155911Z"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_base = 'http://aitools.cs.vt.edu:7860/openai/v1'\n",
    "openai.api_key = 'aitools'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T18:15:24.800028684Z",
     "start_time": "2023-09-07T18:15:24.795727853Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "\tmessages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "\tresponse = openai.ChatCompletion.create(\n",
    "\t\tmodel=model,\n",
    "\t\tmessages=messages,\n",
    "\t\ttemperature=0,  # this is the degree of randomness of the model's output\n",
    "\t)\n",
    "\treturn response.choices[0].message[\"content\"]\n",
    "\n",
    "\n",
    "def converse(prompt, messages=None, model=\"gpt-3.5-turbo\", max_tokens=3000, temperature=0, top_p=1, frequency_penalty=0,\n",
    "\t\t\t presence_penalty=0):\n",
    "\t# Add the user's message to the list of messages\n",
    "\tif messages is None:\n",
    "\t\tmessages = []\n",
    "\n",
    "\tmessages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "\tresponse = openai.ChatCompletion.create(\n",
    "\t\tmodel=model,\n",
    "\t\tmessages=messages,\n",
    "\t\ttemperature=temperature,\n",
    "\t\tmax_tokens=max_tokens,\n",
    "\t\ttop_p=top_p,\n",
    "\t\tfrequency_penalty=frequency_penalty,\n",
    "\t\tpresence_penalty=presence_penalty,\n",
    "\t).choices[0].message[\"content\"]\n",
    "\n",
    "\t# Add the assistant's message to the list of messages\n",
    "\tmessages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "\treturn response, messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple chat\n",
    "\n",
    "Feel free to remove these cells before you export your chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T18:15:27.336679729Z",
     "start_time": "2023-09-07T18:15:27.331728596Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = f\"How are you feeling today?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T18:15:29.660528226Z",
     "start_time": "2023-09-07T18:15:28.655553338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I don't have feelings, but I'm here to help you with any questions or tasks you have. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Conversation\n",
    "Let's maintain a conversation history and see how the model responds to it.\n",
    "Feel free to remove these cells before you export your chat.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T18:15:32.450246317Z",
     "start_time": "2023-09-07T18:15:31.223591561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Hi my name is Jennifer.\n",
      "LLM: Hello Jennifer! How can I assist you today?\n",
      "You: What is my name?\n",
      "LLM: Your name is Jennifer.\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "prompt = f\"Hi my name is Jennifer.\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(f\"You: {prompt}\\nLLM: {response}\")\n",
    "prompt = f\"What is my name?\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(f\"You: {prompt}\\nLLM: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T18:15:33.474446811Z",
     "start_time": "2023-09-07T18:15:33.021811696Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt, messages = converse(\"What is my name?\", messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T18:15:40.722370495Z",
     "start_time": "2023-09-07T18:15:34.073279493Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Assuming yourself as a python developer develop code for a unsupervised machine learning project.\n",
    "follow the instructions below to generate the code of the project\n",
    "\n",
    "```\n",
    "# Load all data files matching 'data/datasets/train-00000-5k-2*.parquet'.\n",
    "# Merge those dataframes into a single dataframe \"df\".\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "(response,messages) = converse(prompt,messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's an example code snippet in Python that demonstrates how to load and merge multiple Parquet files into a single dataframe using the pandas library:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import glob\n",
      "\n",
      "# Define the path to the directory containing the Parquet files\n",
      "file_path = 'data/datasets/train-00000-5k-2*.parquet'\n",
      "\n",
      "# Get a list of all file paths matching the pattern\n",
      "file_list = glob.glob(file_path)\n",
      "\n",
      "# Initialize an empty list to store the dataframes\n",
      "dfs = []\n",
      "\n",
      "# Iterate over each file path and load the corresponding dataframe\n",
      "for file in file_list:\n",
      "    df = pd.read_parquet(file)\n",
      "    dfs.append(df)\n",
      "\n",
      "# Merge all dataframes into a single dataframe\n",
      "df = pd.concat(dfs)\n",
      "\n",
      "# Now you can perform further operations on the merged dataframe\n",
      "# such as preprocessing, feature engineering, or unsupervised learning algorithms\n",
      "```\n",
      "\n",
      "Make sure to replace `'data/datasets/train-00000-5k-2*.parquet'` with the actual path to your Parquet files. This code will load all the Parquet files matching the pattern and merge them into a single dataframe named `df`. You can then proceed with your unsupervised machine learning tasks using the merged dataframe.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{response}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T18:15:40.828731821Z",
     "start_time": "2023-09-07T18:15:40.827778927Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Continue generating code for your project\n",
    "\n",
    "```\n",
    "# Define the code snippets (from data frames's \"code\" attribute)\n",
    "# Define labels (from dataframe's \"language\" attribute)\n",
    "\n",
    "# Convert code snippets to TF-IDF features\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "response,messages = converse(prompt,messages)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T18:15:48.705097791Z",
     "start_time": "2023-09-07T18:15:42.128055840Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's the continuation of the code snippet for your unsupervised machine learning project:\n",
      "\n",
      "```python\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "# Extract the code snippets from the \"code\" attribute of the dataframe\n",
      "code_snippets = df['code'].values\n",
      "\n",
      "# Extract the labels from the \"language\" attribute of the dataframe\n",
      "labels = df['language'].values\n",
      "\n",
      "# Convert code snippets to TF-IDF features\n",
      "vectorizer = TfidfVectorizer()\n",
      "features = vectorizer.fit_transform(code_snippets)\n",
      "```\n",
      "\n",
      "In this code snippet, we use the `TfidfVectorizer` from the `sklearn.feature_extraction.text` module to convert the code snippets into TF-IDF features. We extract the code snippets from the \"code\" attribute of the dataframe and store them in the `code_snippets` variable. Similarly, we extract the labels from the \"language\" attribute of the dataframe and store them in the `labels` variable.\n",
      "\n",
      "Then, we initialize a `TfidfVectorizer` object and use its `fit_transform` method to convert the code snippets into TF-IDF features. The resulting features are stored in the `features` variable.\n",
      "\n",
      "You can now use the `features` and `labels` variables for further analysis or unsupervised machine learning tasks.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{response}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T18:15:48.748347349Z",
     "start_time": "2023-09-07T18:15:48.747786054Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Continue generating code for your project\n",
    "\n",
    "```\n",
    "# Perform K-Means clustering\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "response,messages = converse(prompt,messages)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T18:16:09.801190886Z",
     "start_time": "2023-09-07T18:16:03.086182956Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's the continuation of the code snippet for your unsupervised machine learning project:\n",
      "\n",
      "```python\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Perform K-Means clustering\n",
      "num_clusters = 5  # Define the number of clusters you want to create\n",
      "kmeans = KMeans(n_clusters=num_clusters)\n",
      "kmeans.fit(features)\n",
      "\n",
      "# Get the cluster labels for each data point\n",
      "cluster_labels = kmeans.labels_\n",
      "```\n",
      "\n",
      "In this code snippet, we use the `KMeans` class from the `sklearn.cluster` module to perform K-Means clustering on the TF-IDF features. We define the number of clusters we want to create using the `num_clusters` variable.\n",
      "\n",
      "Then, we initialize a `KMeans` object with the specified number of clusters and call its `fit` method to perform the clustering. The `fit` method takes the TF-IDF features as input.\n",
      "\n",
      "After the clustering is performed, we can obtain the cluster labels for each data point using the `labels_` attribute of the `KMeans` object. These cluster labels are stored in the `cluster_labels` variable.\n",
      "\n",
      "You can now use the `cluster_labels` variable for further analysis or evaluation of the clustering results.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{response}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T18:16:09.818460835Z",
     "start_time": "2023-09-07T18:16:09.809569031Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Continue generating code for your project\n",
    "\n",
    "```\n",
    "#for each cluster, output the cluster labels, along with 5 samples of code snippets from that cluster\n",
    "# Print the cluster assignments with each cluster being truncated to 50 characters long\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "response,messages = converse(prompt,messages)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T18:16:20.721847574Z",
     "start_time": "2023-09-07T18:16:11.851786619Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's the continuation of the code snippet for your unsupervised machine learning project:\n",
      "\n",
      "```python\n",
      "# For each cluster, output the cluster labels, along with 5 samples of code snippets from that cluster\n",
      "for cluster_id in range(num_clusters):\n",
      "    cluster_samples = code_snippets[cluster_labels == cluster_id][:5]\n",
      "    cluster_samples = [sample[:50] + '...' if len(sample) > 50 else sample for sample in cluster_samples]\n",
      "    print(f\"Cluster {cluster_id}:\")\n",
      "    print(\"Cluster Labels:\", labels[cluster_labels == cluster_id])\n",
      "    print(\"Code Snippets:\")\n",
      "    for sample in cluster_samples:\n",
      "        print(sample)\n",
      "    print()\n",
      "\n",
      "# Print the cluster assignments with each cluster being truncated to 50 characters long\n",
      "cluster_assignments = [str(label)[:50] + '...' if len(str(label)) > 50 else str(label) for label in cluster_labels]\n",
      "print(\"Cluster Assignments:\")\n",
      "print(cluster_assignments)\n",
      "```\n",
      "\n",
      "In this code snippet, we iterate over each cluster and output the cluster labels along with 5 samples of code snippets from that cluster. We use boolean indexing to filter the code snippets and labels based on the cluster assignments.\n",
      "\n",
      "For each cluster, we truncate the code snippets to a maximum of 50 characters using a list comprehension. This is done to ensure the output is not too long. We then print the cluster ID, cluster labels, and the code snippets.\n",
      "\n",
      "After that, we create a list of truncated cluster assignments by converting the cluster labels to strings and truncating them to a maximum of 50 characters. Finally, we print the cluster assignments.\n",
      "\n",
      "This will allow you to inspect the cluster assignments and get a glimpse of the code snippets within each cluster.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{response}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T18:16:22.108059131Z",
     "start_time": "2023-09-07T18:16:22.101099425Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
